
\chapter{Future Work}


\section{Dynamic Web Server Implementation}
When utilizing a standard (non-custom), static web server, it is not feasible to allow the user to select new paths based off of any frame except for the first: the already-high storage requirements would increase by a factor equal to the number of frames! As mentioned prior, this problem could be solved by writing a custom web service back-end that could trace the optical flow path dynamically in response to user input; this would require the use of a non-standard web server, and would require additional storage for the binary optical-flow data.

\section{Alternative Data Storage Methods}
There are at least two other implementation possibilities that could be explored to yield savings in storage space:
\begin{enumerate}
    \item the utilization of statically served compressed video as a mechanism for flow data storage; and
    \item expression of flow on a blob basis rather than pixel.
\end{enumerate}

\subsection{Utilization of Video Compression}
It may be possible to utilize widespread browser-compatible video compression technologies to store the entire optical flow dataset and transmit it to the client.

\subsubsection{Benefits}
\begin{itemize}
\item Requires only a static web server.
\item Client download bandwidth requirements should be roughly double of the standard video file alone.
\end{itemize}

\subsubsection{Detriments}
\begin{itemize}
\item May be too noisy to be of use due to chrominance plane spatial subsampling and frequency domain quantization, and especially the cumulative error of iterating over such noisy data;
\item Would be cumbersome to extract data from on the client in a timely manner; and
\item Would demand more computational power for video decompression, challenging the resources required for playback of the original video, which could cause disruptions in real-time behavior.
\end{itemize}

\subsection{Blob Flow}
Instead of a per-pixel basis, selection and navigation could be performed using blob information. There would be many challenges in successfully extracting and tracking semantically meaningful blobs through a scene, but if achieved, the data requirements would drop to a mere fraction of the current storage requirements.


\section{Mobile Device Support}
The Web now enjoys a substantial user base using mobile platforms, such as smartphones or tablets. There exists an opportunity to expand this project's client interface to support such devices. While performance optimizations (discussed below) are particularly beneficial on mobile systems, several unique challenges present themselves:


\subsection{Fat Finger Problem}
    The so-called ``Fat Finger Problem", where a user's finger occludes the very area of interest on the screen that the user is attempting to interact with, preventing fine manual interaction.\cite{fatfinger}

\subsection{Motion Sensors}
A majority of mobile devices contain \emph{motion sensors}\cite{motion}, which generally constitute some combination of:
    \begin{itemize}
    \item Gyroscopes, which can detect changes in angular orientation;
    \item Accelerometer, which can detect translational acceleration, including determination of the Gravity Vector; and
    \item Magnetometer, which can be used as a compass to detect an absolute bearing, relative to Magnetic North.
    \end{itemize}
    Often, the information from the above sensors is passed through \emph{motion fusion}\cite{fusion} algorithms, in order to negate the individual weaknesses of any one variety of sensor, and derive between them all an absolute attitude (or orientation) of the device in real space.\par
    Access to this orientation information is provided through the HTML DOM, and can be used as a novel means of user interactivity. For example, the user could physically rotate the device in order to rotate the video and its motion trajectory path, instead of dragging the mouse.



\section{Optimization}

\subsection{Server Data Reduction}
    Our implementation of a WWW 3D DMVN system requires a large amount of stored data on the server, as described earlier. This represents a meaningful cost to the host of such a DMVN service, and also has a significant impact on the overall network traffic between server and client. We have already explored and implemented several opportunities to reduce that burden:
\begin {itemize}
    \item Elision of the $z$-axis information (Section \ref{elision}) of the path, instead relying on redundant client-side recalculation.
    \item Paring of motionless pixel path information (Section \ref{motionless}), by first determining that a path is not very semantically interesting or useful, and disallowing client-side selection.
\end {itemize}
There are a few more possibilities to reduce the storage requirements:

\subsubsection{Storage Format}
JSON was selected for its JavaScript-native deserialization support, but as a text format it is very verbose. A significant amount of space could be saved by transitioning to a binary alternative, such as \emph{BSON} (Binary JSON).\cite{bson}

\subsubsection{Path Resolution}
    Currently, we supply one path vertex per frame, or the maximum possible path resolution. This may not be practically necessary; a vertex every second or even fifth frame may suffice for an acceptable user experience.


\subsection{Client Performance}
Since user selection of a point along the path (path vertex with nearest temporally-weighted Euclidean distance) is a \emph{reduction}\cite{reduction} operation over the set of vertices, it is ill suited for GPU implementation. Hence, we resolved to perform the per-frame matrix-multiplication (transformation) of the path vertices on the CPU, and then send those to the GPU for display, since otherwise the transformed vertices necessary for user-mouse interaction would be redundantly calculate.\par
In other words, since this fairly heavyweight calculation was necessary to compute on the CPU, we decided not to \emph{also} reperform the computation on the GPU; the transformed data is instead sent to the GPU. However, there is an advanced feature of OpenGL called \emph{Transform Feedback} that would allow the transformed vertices from the GPU to be made available in a buffer accessible to the CPU. The end-result would be identical in that redundant calculation is avoided, but instead of the singular calculation happening on the CPU, it would happen on the GPU instead, which should be more performant.



\section{Semantics \& Intuitiveness}

\subsection{Semantically Interesting Motion}
    The fundamental purpose of DMVN is to allow for a more intuitive video-navigation method: the user is enabled to drag an ``object of interest" through space to navigate to the corresponding time in the video. These ``objects of interest" may often be people, animals, or objects. However, motion beyond of these ``objects of interest" would generally serve to cloud the user experience, especially in situations that may challenge conventional computer vision techniques, or otherwise cannot be resolved to a singular, clear path of motion. For example, fluids such as smoke, rain, rustling leaves, bodies of water, or perhaps even clouds do not exhibit the type of motion that we wish to capture and express to the user as a ``motion path" for the purposes of navigation.
\par We can (inexhaustively) categorize these kinds of unwanted motion as follows:
\begin{itemize}
\item Incoherent Motion; and
\item Global Motion.
\end{itemize}

\subsubsection{Incoherent Motion}
    The motion of leaves rustling, smoke billowing, sea foam. To address this, I might:
    \begin{enumerate}
    \item segment the optical flow image into blobs of areas of coherent motion, or
    \item otherwise perform some statistical analysis of pixel neighborhoods to filter out motion that doesn't show cohesive motion over a relatively large pixel area
    \end{enumerate}
    This would of course risk filtering out smaller objects that \emph{would be} semantically interesting to track, such as a flying bird in the distance.
\subsubsection{Global Motion}
    ``Global motion" covers issues such as:
    \begin{itemize}
    \item camera motion, either rotational, translational, or both;
    \item rain, fog, mist, or other atmospheric motion (this also applies if the video is entirely of a body of water); and
    \item noise, such as that from the video source (CCD/CMOS image sensor or film) or from compression.
    \end{itemize}

\subsection{Immobile Regions}
In our example videos, and in many others where the camera is immobile, a majority of the video may be considered to either be an ``immobile region": when the user selects a pixel to generate a motion-path, they will be greeted with what appears to be a straight line. This may cause confusion to the user, as they have been told that DMVN helps them navigate by dragging objects, and yet selecting an area that is not an object causes what may appear to them to be confusing behavior.

\subsection{Camera Motion}
Camera motion provides some unique challenges to DMVN; the examples demonstrating DMVN avoid this issue, because it is difficult to overcome. Since the intent of the motion path is to be able to visualize the motion of the object throughout a scene, the true path would become confusing if the frame of reference of that motion were to change mid-stream.\par
    While solutions exist for camera pose and motion estimation (and for the related field of video stabilization), how can these techniques be used in the context of DMVN? I would propose experimentation of the following scheme:\par
    Basic 3D DMVN allows the user to rotate the video and path in the $x$- and $y$-axes; the video planar section otherwise stays at the origin (translationally), and is not rotated in the $z$-axis. In an enhanced model, when the camera undergoes some transformation (translation, rotation, etc), then the video planar section could counter-transform in the opposite direction, but same amount, in order to achieve a kind of ``path stabilization", to ensure that the path's frame of reference remains constant throughout the scene.

\subsection{Semantic Segmentation}
The premise of DMVN is the ability to navigate a video by dragging an object through its motion path; not a pixel and its motion path, as I've implemented it. In order to improve this aspect, there are several questions that need to be answered:
\begin{itemize}
    \item What are the objects under motion?
    \item What are the objects under motion \emph{that are semantically interesting}?
    \item What kinds of motion are interesting enough to use as a means of video navigation?
    \item What \emph{parts} of an object are worth tracking?
\end{itemize}
This would be more of an artificial intelligence research area. If the number of semantically interesting paths could be automatically detected and simplified to one or two, then DMVN becomes trivial for both the user and for the host (in terms of reduced data storage needs, user-interactivity requirements, etc).
\par This can be very challenging, because for example, a person may or may not be one object to track: their hands and legs may be moving in different directions that may be interesting of their own accord, or other times the user may be interested in the path of the whole person.

\subsection{Scene-Change Detection}
    Many videos have scene changes, especially those which have been post-produced from a collection of ``takes", or live-broadcast videos where there is a mechanism for a Producer to switch between cameras. Generally, it would probably be wise to reset the DMVN path tracking when a scene change is detected.\par
    There may, however, be some instances, where it might be possible to correlate the path between two scenes. As an example, let us imagine a guitar tutorial video: the camera is positioned to show the guitarist's hand positioned on the fretboard so that the viewer can see and analyze the fingerings. The scene changes to a camera that is from the guitarist's point of view, so that the viewer can understand how the fingering might look from their own perspective. It could be possible to track the path of the finger between the two scenes.\par
    If an affine transform of the camera's scene change can be derived, then it may be possible to keep the finger's path as a continuous line, but to perform that affine transform on the video's rectangle.

