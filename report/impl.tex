
\chapter{Implementation}


\section{Systems Architecture}

\subsection {Considerations}
In the analysis of the systems architecture for this implementation, we must give prime consideration to which aspects of a WWW 3D DMVN differ from those of a native PC implementation. These aspects include (but are not limited to):
\begin{itemize}
\item Network Bandwidth;
\item Network Latency;
\item Client Performance; and
\item Server Burden.
\end{itemize}

\subsubsection{Network Bandwidth}
Network bandwidth is an important consideration in the design of a web-based system that is generally not a factor for native programs that operate locally on a single PC. Insufficient consideration for this will cause the client to experience discouragingly long waits while information is transmitted to their PC over the Internet, and will cost the proprietors of the server money, as they will need to increase their upload bandwidth service in order to avoid service congestion.\cite{responsetime}
\par It is thus paramount to minimize the amount of data that must be transmitted to the PC to be the absolute minimum necessary.

\subsubsection{Network Latency}
Similarly, client-server transactions will always take some amount of time, and so client dependency on a large number of small transactions should also be avoided, due to the detrimental effects on interactivity that network latency will cause.
\par For example, we should probably avoid a DMVN system where the nearest path vertex to a mouse click is detected by a request from the client to the server, where the answer is calculated there and returned to the client over the internet. This sort of architecture would cause an unresponsive user experience due to the delayed interactivity.\cite{responsetime}

\subsubsection{Client Performance}
Since web clients can run on all sorts of computers, with various browsers and hardware capabilities, we should be cautious about making assumptions about the performance capabilities of the client. This again is different from writing a native application, where it may be possible to control which computer hardware it is executed on.
\par In general, to provide a positive, low-latency user experience, we want to keep client run-time performance a prime consideration, and be mindful of the potential impact of intense calculations and graphics on such.

\subsubsection{Server (Back-End) Burden}
Back-end server burden can take the form of:
\begin{itemize}
    \item Data Storage
    \item Custom HTTP Services
    \item Computation (CPU Cycles)
    \item Bandwidth (as mentioned above)
\end{itemize}
All of the above incur monetary cost to the server host, and so should all be considered when designing a Web system.


\subsection {Architecture}
The client-facing front-end web page will provide all input/output user interactivity: it will show the 3D DMVN video and path, it will provide the means for the user to rotate the DMVN video and navigate through the motion trajectory path. It will load its requisite front-end files: HTML, CSS, JS (JavaScript), and video (H.264/MP4) are statically requested from the server by the web page, as is standard and expected by any HTML page on the World Wide Web.
\par However, since path data for every pixel is too large to make a static resource of the web page, we will instead make dynamic HTTP GET requests for that data, and only send the necessary information across the Net. In order to achieve this via standard HTTP services (and not require a custom back-end server), we will make a \emph{separate path data file} for each pixel in each video.
\par On the Web front-end, we will provide interactivity through JavaScript and use GPU acceleration through the WebGL API for the graphics processing required for 3D DMVN.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{arch}
\caption{Client-Server 3D DMVN Architecture (HTTP Requests)}
\end{figure}

\section{Back-End Preparation}
Before we can implement the 3D DMVN front-end service, we need to first generate path data files from the provided video data.

\subsection{Optical Flow}
Optical Flow is a means of determining the motion of pixels from one frame of video to another. The binary files associated with the provided videos are optical flow data, providing a $\Delta \vec{p} = (\Delta x, \Delta y)$ for each pixel, for each frame of video. This $\Delta \vec{p}$ expresses how the optical flow algorithm determined the pixel from the first frame to the next.\cite{opticalflow}
\par We intend on using this optical flow information to trace the path of motion from a starting pixel through each frame. We can define this algorithm recursively:
\begin{equation}
    \vec{p}_n = \vec{p}_{n-1} + \vec{\Delta p}_n(\vec{p}_{n-1}) \\
\end{equation}
where the initial pixel position $\vec{p}_0$ is given, and $\vec{\Delta p}_n(\vec{p})$ is the optical flow function for frame $n$ at position $\vec{p}$.

\subsubsection{Bilinear Interpolation for Subpixel Optical Flow Parameter}

While it is always the case that our initial position is $\vec{p_0} \in \mathbb{N}$, $\vec{f_n}(\vec{p_{n-1}}) \in \mathbb{R}$. In other words, the optical flow field is only defined for pixels (with integral coordinates) and not subpixels (with real coordinates), so in order to find $\vec{f_n}(\vec{p} \in \mathbb{R})$, we must perform interpolation between several $\vec{f_n}(\vec{p} \in \mathbb{N})$.
\par The form of interpolation we choose is Bilinear Interpolation, because it is computationally simple while providing sufficient quality of results. In order to do this at $\vec{p} \in \mathbb{R}$, we will first fetch the four surrounding defined optical flow points $\vec{g}_n$:
\begin{align*}
    \vec{g}_1 & = \vec{f}(\langle \lfloor p_x \rfloor, \lfloor p_y \rfloor \rangle) \\
    \vec{g}_2 & = \vec{f}(\langle \lfloor p_x \rfloor, \lceil p_y \rceil \rangle) \\
    \vec{g}_3 & = \vec{f}(\langle \lceil p_x \rceil, \lfloor p_y \rfloor \rangle) \\
    \vec{g}_4 & = \vec{f}(\langle \lceil p_x \rceil, \lceil p_y \rceil \rangle)
\end{align*}
and then perform Linear Interpolation as follows:
\begin{align*}
    \vec{a} & = \text{lerp} (\vec{g}_3, \vec{g}_1, \{p_x\}) \\
    \vec{b} & = \text{lerp} (\vec{g}_4, \vec{g}_2, \{p_x\}) \\
    \vec{f}_{interp} & = \text{lerp} (\vec{b}, \vec{a}, \{p_y\})
\end{align*}
where $\{x\}$ is the fractional portion of $x$, or $\{x\} = x - \lfloor x \rfloor$, and Linear Interpolation (lerp) is defined as:
\begin{equation}
    \text{lerp}(\vec{a}, \vec{b}, x) = x\vec{a} + (1-x)\vec{b}
\end{equation}
where $0 \leq x \leq 1$.

\subsection{Path Generation}
Using a program we wrote in \texttt{python} with the \texttt{numpy} and \texttt{opencv} libraries, we generated path information for the web page to use. Ultimately, we intend to export the generated path as JSON information; in order to do so, we will convert the set of path coordinates from Video (Pixel) Space (Section \ref{videospc}) to Normalized Device Coordinates (Section \ref{ndcspace}), with the $z$ coordinate set to be on $[0,1]$, where 0 is the first frame, and 1 is the final frame.

\subsubsection{Interactive Path Generation}
The first program, written for development purposes, would load the first frame of the video and allow a user to select a single pixel with a mouse click. Then, it would trace the optical flow from the pixel, showing its path while playing back the video. Finally, the program provided JSON for the selected pixel's path; we used this for initial development and testing.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Interactive}
\caption{Interactive Flow Path Generation (Small, Cyan Circle Near Center)}
\end{figure}

\subsubsection{Batch Path Generation}
The second program does the same as above, eschewing the visualizations and UI for performance, tracing the optical flow path for every pixel in the video (rather than a single pixel selected by a user). After some optimization by minimizing looped calculations and preloading all optical-flow files into memory, We were able to get execution time for the batch path generation down to about 12 seconds per video line for the \texttt{gym1} video, and about 15 seconds per line for the \texttt{painting1} video, which . However, it would still take more than an hour to generate the paths for every pixel in these relatively short and low-resolution videos. The final result of running this program was a directory full of JSON path files, with one file per pixel and a strict naming format: for example, \texttt{painting1-0235x0192.json}.




\section{World Wide Web}
\subsection{Relevant Technologies}
Our solution will be formed by a combined system of several discrete technologies.
\subsubsection{HTTP}
\emph{HyperText Transfer Protocol} is an Application Layer (OSI) network protocol that serves as a foundation for the World Wide Web. Through it, a client (Web Browser) can request (``GET"), send (``POST"), or otherwise transceive data to or from the Web Server. HTTP is the core of our client-server system.\cite{http}
\subsubsection{HTML}
\emph{HyperText Markup Language} describes the content of a web page. This was the original World Wide Web technology, and it continues to form the backbone today, allowing the author to list and name which DOM elements are present, and in what hierarchy.\cite{html}
\subsubsection{CSS}
Through \emph{Cascading StyleSheets}, the styling of HTML elements can be managed; CSS was introduced to separate the concerns of content (HTML) and presentation (CSS), so that each could be modified independently of the other.\cite{css}
\subsubsection{DOM}
\emph{DOM} (Document Object Model) is the API by which JavaScript can access and interact with the client's web page, the front-end for our application. Through the DOM, JavaScript can add event handlers; add, modify, or remove elements from the HTML tree structure; and change the styling and behavior of the webpage as a whole.\cite{dom}
\subsubsection{JSON}
\emph{JSON} (JavaScript Object Notation) is a format for structured data storage in an ASCII (as opposed to binary) representation.\cite{json} Its use is particularly popular in conjunction with JavaScript (and by association, the World Wide Web as a whole), since it is able to serialize and deserialize native JavaScript objects. Its primary weakness is verbosity (file size), especially when compared with a binary format. It is, however, less verbose than XML, which it has come to replace in most recent projects.\par
In our project, JSON is the format in which we store path data on the server; the client can request the JSON from the server via HTTP requests, and deserialize the data directly into a JavaScript objects for programmatic use.
\subsubsection{JavaScript}
\emph{JavaScript}, also known by its proper name \emph{ECMAScript}, is the sole native scripting language of the World Wide Web. When adding programmatic capabilities to a website, JavaScript is the only allowed language.\cite{javascript}
\subsubsection{WebGL}
\emph{WebGL} is the API by which a client webpage can utilize the computer's GPU for graphics coprocessing. A standard from the Khronos Institute, it is derived from OpenGL ES 2.0, which is itself a subset of OpenGL intended for simpler mobile devices.\cite{webgl} \par
Interestingly, despite its implementation in JavaScript WebGL's API remains nearly identical to OpenGL's relatively archaic and obtuse C API: no high-level constructs or programming methodologies (such as Object Oriented or Functional Programming) are employed.

\subsection{User Interface}
\label{ui}
For our implementation, we decided on a simplistic user interface: the only visible element of the web page is the 3D DMVN video. The video automatically begins playing on startup and whenever the video is manually temporally navigated, in order to remove the necessity for an extraneous ``play" button.
\par The user interacts with the 3D DMVN video through the mouse alone. These user-interface decisions were made in order to keep the user's focus entirely on the 3D DMVN navigation experience, although they limit the usable devices to personal computers; a user-interface for mobile devices such as smartphones or tablets requires many particular accommodations to overcome inherent complications from touch interaction and small screen sizes. Laptops with only a trackpad may not have middle mouse-button functionality; in this case, an external mouse peripheral would be necessary. We'll make the claim that the personal-computer interface is the most well-established, and thus detracts the least from the user-interface of the 3D DMVN itself.

\subsubsection{Right Mouse Button}
\label{uirmb}
Holding down the right mouse button and dragging the mouse causes the video's angle of view (rotation) to change up to a preset maximum angle per axis: mouse motion to the left or right induces yaw, and motion up or down causes pitch to change. Rolling of the video (rotation about the $z$-axis) was determined not to be useful in disambiguating the navigation path, and would indeed be likely to cause confusion.
\par There is currently no way to ``reset" the video to a neutral position. It is our judgement that adding a new interface to do so would overcomplicate the user experience; rather, we would propose this: when a user does not interact with the video for some period of time, the video's rotation slowly starts to return back to neutral.
\par 

\subsubsection{Left Mouse Button}
\label{uilmb}
When the user pressed the left mouse button, the current location of the mouse cursor (pointer) on the screen relative to the current motion path is used to change the seek time of the video.
\par The mechanism used to correlate mouse-cursor screen-space position with video seek-time is explained in Section \ref{temporalnav}.

\subsubsection{Middle Mouse Button}
\label{uimmb}
Pressing the middle mouse button down allows to selection of a new object (or pixel) to begin navigation by. The pixel of the video that the mouse cursor was over when the middle mouse button is lifted becomes the pixel from which the new motion trajectory path is generated.
\par In the static server implementation of 3D Web DMVN, paths can only be generated based on pixel coordinates of the \emph{first} video frame, so when the middle mouse button is pressed down, the video immediately seeks to the first frame so that the pixel selection can be meaningful to the user; on release of the button, the selected pixel is calculated (regardless of current video rotation or orientation) as described in Section \ref{raytracing}.


\section{Hardware-Accelerated Graphics}
In order to realize the performance necessary to display the 3D DMVN interface in real-time to the client, we will need to utilize Graphics Processing Unit (GPU) hardware.

\subsection{GPU APIs}
GPUs must be controlled through an API, which acts as an interface between the CPU and the GPU's device driver. While there are several such APIs, the most widespread by far is known as \emph{OpenGL}.\cite{opengl} OpenGL was developed in the 1990s, and today remains an archaic and challenging stateful C API.
\par \emph{WebGL} is an implementation of OpenGL meant to enable access to GPU hardware in web browsers. It is based off of the OpenGL ES 2.0 version\cite{opengles2}, which is a more recent, somewhat limited version of OpenGL for mobile devices. Ironically, even though WebGL is a JavaScript API (since JavaScript is the only language available natively in a web browser), its API is almost identical to the archaic C API. Most web developers who do not have knowledge in computer graphics might add 3D graphics to their websites by using a wrapper library around OpenGL, such as the popular \texttt{three.js}, but for the purposes of our project, we will stay as close to ``bare metal" as possible for educational purposes, and use WebGL directly.

\subsection{OpenGL Architecture}
While a full description of OpenGL architecture is not within the scope of this report, a cursory review may be warranted. When using WebGL (which is, for all intents and purposes, the same as OpenGL), we must perform substantial setup, or write ``boilerplate" code to create the graphics environment, which consists of the following objects:\cite{opengles2}
\begin{itemize}
    \item \emph{Programs}, which are made of compiled and linked \emph{Shaders}, written in the GLSL language;
    \item \emph{Buffers}, which are arrays of data to be considered Vertex Attributes (attributes correlated to a vertex), such as position or texture coordinates;
    \item \emph{Textures}, which are (generally) 2D image bitmaps.
\end{itemize}
\par When a \emph{draw} function call is made on a Buffer, then the following basic process takes place:
\begin{enumerate}
    \item The \emph{Vertex Shader} executes \emph{for each vertex being drawn}; the vertex shader takes as input the following:
    \begin{itemize}
        \item \emph{Attributes}, which are information that is correlated with the vertex and had been stored in a Buffer;
        \item \emph{Uniforms}, which are \emph{not} per-vertex information, but global to all vertices.
    \end{itemize}
    and outputs the following:
    \begin{itemize}
        \item \emph{Varyings}, which are information correlated to the current vertex that will be passed on to the Fragment (Pixel) Shader;
        \item \emph{Position}, in Normalized Device Coordinates, which will be used in the Clipping stage.
    \end{itemize}
    \item In the \emph{Clipping} Stage, any drawn triangles where all vertices have at least one of their $x$, $y$, or $z$ coordinates of their Position out of the bounds [-1,1] will be clipped and will not progress any further;
    \item \emph{Rasterization}, where unclipped polygons will be rasterized to the viewport based off of their vertices' Positions;
    \item The \emph{Fragment Shader}, also known as a Pixel Shader, which will be run independently for each of the pixels that were rasterized for the polygon. It takes as inputs the following:
    \begin{itemize}
        \item Varyings, which are bilinearly interpolated from the values set by the vertices based off of the spatial position of the fragment (pixel) relative to those vertices;
        \item Uniforms, which again are global information shared among all fragments.
    \end{itemize}
    and as outputs the following:
    \begin{itemize}
        \item Fragment Color, which is the RGBA value that the pixel will ultimately be drawn as.
    \end{itemize}
\end{enumerate}

\section{3D DMVN WebGL Shader Programs}
In our project, we need three different shader programs, for three different elements that must be drawn in different ways:
\begin{enumerate}
    \item The video planar section (drawn as two adjacent triangles forming a ``quad");
    \item The wire-frame box volume (drawn as lines); and
    \item The motion trajectory path (drawn as a line strip [continuous line segments])
\end{enumerate}

\subsection{Video Planar-Section Shader}
The Video Planar-Section Shader shows the rotated rectangle with current video frame.

\subsubsection{Buffers \& Attributes}
The rectangle is defined as six 3D Position points, with each set of three comprising a single triangle primitive. We define the points to cover the entirety of the $xy$ plane in the Normalized Device Coordinate space, with $z=0$.
\par There are also six 2D texture coordinate points defined to correspond to the six 3D Position point data. These are defined within the Normalized Texture Coordinate space, with $\langle 0,0 \rangle$ in the upper-left corner, and $\langle 1,1 \rangle$ in the lower-right.

\subsubsection{Textures}
The video frame is the only Texture used in our project, and it is updated (re-uploaded to the GPU) before every single draw call to ensure that it reflects the contents of the hidden \texttt{video} DOM element.

\subsubsection{Uniforms}
The only uniform, aside from the Texture Sampler, is the 3x3 transformation matrix, covered later in the Mathematics chapter. Unlike the other two shader programs, this transformation is applied without a prior shift along the $z$-axis, as the video is meant to always be positioned in the center.

\subsubsection{Vertex Shader}
The Vertex Shader simply applies the transformation matrix to each vertex, and reports that Position to the Clipping and Rasterization stages. Since depth buffering is not utilized, we report the Position as the projection onto the $xy$ plane in Normalized Device Coordinates, and pass the $z$ coordinate as a varying to the Fragment Shader for our own use.

\subsubsection{Fragment Shader}
While the Fragment Shader is also very basic, largely performing the operation of sampling the texture and pushing that sample's color as its output, it does take the $z$ coordinate passed from the Vertex Shader stage into account to brighten or darken the pixel slightly based on its depth.
\par Since 3D DMVN uses orthographic projection, we decided that some indication of what is near or far in lieu of perspective might be helpful to the user, and so chose to do it through this mechanism or brigthening closer pixels and darkening further pixels.

\subsection{Wire-Frame Box-Volume Shader}
The Wire-Frame Box-Volume shader draws an encapsulating volume to visually reinforce the depth component of the video and path, while the video remains translationally stationary in the center.

\subsubsection{Buffers \& Attributes}
The only attribute of the Box-Volume vertices is the 3D position. Since they are drawn as lines, each primitive consists of a two vertices: a start and end point. Since the box-volume is defined as a rectangular prism, there are 12 line segments needed to draw it, and so a total of 24 vertices are defined.
\par In this case, we take advantage of the full range of positional values in the $xy$ plane, $[-1,1]$, so that the transform matrix will make the $xy$ dimensions of the box the same as the $xy$ dimensions of the video rectangle. We define $z$ on the range of $[0,1]$, so that its untransformed state would be the position it should be at the beginning of the video.

\subsubsection{Uniforms}
While the transformation matrix that is passed to the Video Rectangle shader program is also passed to the Box shader program, the Box also receives a \texttt{time} uniform, that tells it what the current seek time of the video is, on the range of $[0,1]$.

\subsubsection{Vertex Shader}
The Vertex Shader subtracts the value of the \texttt{time} uniform from the $z$ coordinate of the initial vertex position \emph{before} applying the matrix transform. In this way, the box volume appears to slide backwards over time as the video plays, always containing the video rectangle within.

\subsubsection{Fragment Shader}
The Fragment Shader is the simplest possible: it passes through a constant white color with a small alpha value so that the box volume is as unobtrusive as possible.


\subsection{Motion Trajectory Path Shader}
The Motion Trajectory Path Shader draws the motion trajectory ``seek" path over the video.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Path}
\caption{Path line drawn over video rectangle}
\end{figure}

\subsubsection{Buffers \& Attributes}
Unlike the other two shaders, the Path Shader receives vertex position information that is pre-transformed (both a negative $z$-axis shift, as well as with the matrix applied), since those transformations are pre-applied on the CPU for purposes of mouse-proximity calculation, and thus there was no need for redundant calculation. Whether the memory cost of retransmitting the positional buffers every frame is more than the perceived cost of redundant calculation is unverified.

\subsubsection{Vertex Shader}
The Vertex Shader simply reports the position of the vertices as that of the $xy$ projection of the respective positional attribute. The $z$ coordinate of the vertex is passed along to the Fragment Shader as a Varying.

\subsubsection{Fragment Shader}
While the Fragment Shader always reports of the color of the line to be yellow, it does change the alpha (transparency) of the pixel based on whether it is \emph{in front of} or \emph{behind} the video rectangle, to help the user determine depth.

\section{Storage Requirement Reduction}
The na{\"i}ve implementation of our method results in a surprisingly large amount of data (Section \ref{conclusionresults}). We explored some methods of reducing that storage requirement.

\subsection{Motionless Pixels}
    Many pixels do not move significantly throughout a scene. Not only does this cause substantial waste in storing useless data, but it can also be confusing to a user when they click on a pixel and get a simple, straight line. If a path is nearly motionless, it could be either:
    \begin{enumerate}
        \item Noted in that pixel's respective file as a ``motionless pixel", rather than explicitly list every frame's vertex. This would produce effectively the same behavior as presently.
        \item Noted in that pixel's respective file as a ``motionless pixel", with redirection to the nearest (2D Euclidean) pixel that \emph{does} have motion, so that the user does not need to search for semantically useful objects to interact with.
        \item Noted in that pixel's respective file as a ``motionless pixel", causing no generation of a new path when the user attempts to select that pixel, with perhaps some negative feedback (such as a tone or screen blink).
    \end{enumerate}
    \par Of these options, the second may be most intuitive, but its calculation would be too complex for the scope of this project; we have implemented the first and third options, and found the first to be confusing (since there is no ``object" in the scene nor motion to navigate by, the straight line is semantically meaningless) and the third to be more preferable.
    \par The means by which our selected solution is implemented are described in Section \ref{paring}.

\subsection{Elision of $z$ Coordinate}
    The inclusion of the $z$ coordinate for each vertex is not truly necessary, as that information could be reconstructed on the client side by dividing each array element's index by the element count, minus one. This would add a small amount of performance overhead to the client process, but since it only occurs once each time a path is loaded, its overall impact should be minimal for the amount of space saved on the server (nearly $\frac{1}{3}$).

